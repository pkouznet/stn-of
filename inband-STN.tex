%\documentclass[11pt,pdftex,letter]{article}
\documentclass[conference]{sigcomm-alternate}

%\documentclass{sig-alternate-10pt}


%\documentclass[11pt]{llncs}
%\documentclass[11pt,pdftex]{article}
%\documentclass{sig-alternate-10pt}
%\usepackage{amsthm}
%\usepackage{algorithm}
%\usepackage[noend]{algorithmic}


\usepackage{amssymb}
\usepackage{comment}
\usepackage{amsmath}
%\usepackage{graphicx}
%\usepackage{color}
%\usepackage[pdftex]{graphicx}
%\DeclareGraphicsRule{*}{mps}{*}{<++>}


%\usepackage[caption=true,font=footnotesize]{subfig}
%\usepackage{xspace} 	% Guesses whether a space is needed when invoked
\usepackage{cite}
%\usepackage{framed}
\usepackage{framed,color}
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}

%\usepackage{algorithm}
%\usepackage[noend]{algorithmic}

\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\algrenewcommand\algorithmicreturn{\State \textbf{return}}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\algdef{SE}[TRANSACT]{StartTransaction}{EndTransaction}{\algorithmicaaa}[1]{\algorithmicwhile\ #1}%

\algblock[<block>]{<start>}{<end>}
\algblockdefx[Transaction]{startTransaction}{endTransaction} %
[0]{ \textbf{start transaction}} %
[1][res]{ $#1\gets$  \textbf{commit transaction}}

%
%\usepackage{comment}

%[[PKto change spacing
%\usepackage{titlesec}
%\titlespacing\section{0pt}{7pt}{6pt}
%]]

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{takeaway}[theorem]{Takeaway}
%\newtheorem{fact}[theorem]{Fact}


%\pdfpagewidth=8.5in
%\pdfpageheight=11in

% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.


\begin{document}
\sloppy

%\title{Distributed Network Programming}
%\title{The Distributed SDN Update Problem:\\Towards Software Transactional Networks}

%\title{On Consistent Updates in Software Defined Networks under Unreliable Control}
%\title{The Case for Reliable Software Transactional Networking}

%\title{A Distributed SDN Control Plane for Concurrent Policy Updates}

\title{Solving Consensus with OpenFlow}

\title{One-Shot Coordination of Distributed SDN Controllers:\\The Case for Multi-Write Consensus}

\title{One-Shot Coordination of Distributed SDN Controllers:\\On the Consensus Power of OpenFlow}


\author{
Liron Schiff\thanks{Supported by the European Research Council (ERC) Starting Grant no. 259085 and by the Israel Science Foundation Grant no.~1386/11.} $^1$, 
Stefan Schmid$^2$, Petr Kuznetsov$^3$ \\
\small $^1$ Tel Aviv University, Israel; $^2$ TU Berlin \& T-Labs, Germany; $^3$ ParisTech, France
}

%\institute{}

\date{}


\maketitle


\thispagestyle{empty}

%\if \SAVESPACE 1
%\setlength{\floatsep}{3pt}
%\setlength{\textfloatsep}{3pt}
%\setlength{\dbltextfloatsep}{3pt}
%\setlength{\intextsep}{3pt}
%\setlength{\abovecaptionskip}{3pt}
%\fi

% A category with the (minimum) three required fields
%\category{C.2.1}{Network Architecture and Design}{Centralized Networks}
%\category{C.2.4}{Distributed Systems}{Network Operating Systems}
%\terms{Measurement, Performance}
%\keywords{}


\begin{abstract}
The control plane of future Software-Defined Networks (SDNs)
is likely to be \emph{distributed}: a redundant control plane
can ensure a high availability, and a spatially distributed
control plane allows to handle time-critical dataplane
events close to their origin. Moreover, future SDNs may also
allow multiple administrators or participatory users to update
policies concurrently, e.g., by offering transactional interfaces. 
However, not much is known today about how to implement 
a consistent distributed control plane which supports concurrent
policy installations. 

In this paper, we propose to coordinate distributed controllers
directly on the Openflow switches: exploiting classic results
on multi-writer consensus, we show how to use Openflow 
to synchronize controllers and install policies in a consistent
manner, \emph{in one shot}. Our approach is attractive as it
does not require complicated helping mechanisms in case of controller
failures. 
\end{abstract}

%\vspace{1cm}

%\begin{center}
%{\bf [Regular paper only]}
%\end{center}
%[[PK I guess not anymore?]]

%\vspace{1cm}

%\begin{center}
%{\emph{Contact Address:}\\Marco Canini, Place Sainte Barbe~2, 1348 Louvain-la-Neuve, Belgium\\Tel: $+$32 10 47 48 32,
%marco.canini@uclouvain.be}
% {\emph{Contact Address:}\\Stefan Schmid, MAR 4-4, Marchstr.~23, 10587 Berlin, Germany\\Tel: $+$49 175 930 98 75,
% stefan.schmid@tu-berlin.de}
%\end{center}


%\newpage

%\begin{center}
%{\bf Regular and student paper: Dan Levin is a full-time student.}
%\end{center}

%\section*{todo before submission}

\section{Introduction}\label{sec:intro}

By consolidating and outsourcing the control over the dataplane switches to a logically
centralized controller,
Software-defined networks (SDNs) simplify the network management and facilitate faster
innovations, as the (software) control plane is decoupled and can evolve independently.
Moreover, Openflow, the standard SDN protocol today, introduces many generalizations,
in terms of traffic engineering, definition of flows, as well as in-band network functionalities,
by relying on a simple match-action paradigm which allows us to define
forwarding rules based not only on Layer-2, but also Layer-3 and Layer-4 (and possibly beyond)
header fields.
 
While the logically centralized network perspective has the potential to simplify
the network management---many network tasks are inherently non-local---and introduces
many flexibilities,
managing and operating an SDN in a more adaptive but consistent manner is non-trivial, even with SDN.
A particularly interesting problem regards the consistent installation of new policies
or routes.~\cite{todo,todo,todo} While implementing consistent network updates 
is challenging already from the perspective of a single controller, there exists a wide consensus
that the logically centralized SDN control planes will likely be \emph{actually distributed} in the future.

The reason for implementing SDN control planes in a distributed manner are manyfold.
First, in order to provide a high availability as well as a minimal degree of
fault-tolerance, controllers should be redundant~\cite{onix,stn,todo}: a failure
of one controller can be masked by other controllers. Second, it has also been proposed
to distribute controllers \emph{spatially}, in order to handle latency-sensitive and
communication intensive control plane events close to their origin.~\cite{devoflow,kandoo,jukka,disco}
Third, larger SDNs are likely to be operated by multiple administrators or may even offer
participatory interfaces where different users can install and trigger policy changes
concurrently.~\cite{participatory,stn}

However, today, we do not have a good understanding of how to realize
such distributed control planes. The problem is essentially a distributed
computing one: Multiple controllers may simultaneously try to
install conflicting policies, and a consensus must be reached which one of
the two is actually installed. Moreover, controllers may also fail,
and a mechanism must be provided to complete an update in this scenario.

\subsection{Our Contributions}

In this paper, we propose an approach to solve these problems.
In particular, we promote a very direct approach to coordinate controllers,
using the Openflow bundle feature to make an atomic consensus and update:
using one message, a single controller will be able to install its policy,
\emph{in one shot}. Our approach relies on a classic result in 
distributed computing, namely multi-writer consensus. Interestingly,
this result is not well-known even in the distributed computing community,
and we believe that the SDN paradigm offers a very practical
application for it. We believe that our approach nicely complements
existing, more theoretical literature on distributed control planes; for example,
it shows how to realize STN~\cite{stn}.

\subsection{Organization}

The remainder of this paper is organized as follows.
In Section~\ref{sec:background}, we provide the theoretical
background on our approach. Section~\ref{sec:realization}
then discusses the basic implementation of this approach in Openflow.
Section~\ref{sec:extensions} discusses practical issues and extensions.
After reviewing related literature in Section~\ref{sec:relwork},
we conclude our work in
Section~\ref{sec:conclusion}.


\section{The Underlying Theory}\label{sec:background}

idea: multi-write consensus, a not well-known result!

In order to solve consensus we suggest to use the atomic multiple entries update capability (which is discussed in OpenFlow standard). A shared memory system that supports atomic write to (enough) multiple locations is known to allow the implementation of consensus objects. For example consider the following implementation utilizing $n^2$ memory locations, $M_{i,j}$, where $i,j\in[n]$, and the suggested values $\{v_i\}_{i\in [n]}$. All locations are initialized to zero.

In order to participate, a server i, writes its suggested value $V_i$ and atomically rewrites row i ($M_{i,*}$) with value "1" and column i ($M_{*,i}$) with value "2". After the atomic write, a server $i$ first reads the diagonal ($M_{j,j}$ for $j\in [n]$), and consider all observed servers as candidate set S.  Then server i reads all candidates intersections (locations $M_{j,k}$ where $j,k\in S$. And computes the winner server id w that was overwritten by all other candidates, i.e. such that $M_{w,k}=2$ for all $k\in S {w}$. The consensus value is $v_w$.
Note that a snapshot could replace the two reading phases.



\section{Basic One-Touch Mechanism}\label{sec:realization}

We address the problem of committing consistent policies to SDN switch concurrently by multiple controllers.
We show that assuming minimalistic control protocol, SDN switch configuration space can be used to implement a consensus object thereby allowing controllers to agree on each next policy update.

\subsection{SDN switch configuration space as shared memory}





Our first observation is that a match-action configuration which is accessed by multiple controllers can be considered as shared memory. Note however that standard memory map and index/offset to a word, while Flow Table entries are not necessarily indexed. However, we assume that each flow entry has some property that allows the operator to specifically reference and update it, for example OpenFlow entry's cookie. In cases where this property value can't be determined by the operator, we can use other variable entry properties to store the index, for example we can represent memory value $v$ in offset $i$ by an match-action entry whose match is $in\_port = i$ and action is "forward port $x$".
%To overcome this we can use the match part of an entry as the index and the action part as the word value.


Going back to the policy update problem, the consensus scheme allow all servers to decide on next policy by using consensus value as policy identifier, and writing the policy in advance. However this scheme doesn't actually apply the policy on incoming packets and it remains to configure the switch according to the next policy once it is decided. Making this scheme fail safe (handling failure of winning server) can be ensured by allowing any server to reconfigure the switch.

\subsection{ One-Touch}

In some scenarios it may be desirable to shorten the policy update time by avoiding the consensus reading and policy configuration step. We define this as the one touch policy update, namely we require each server to contact the switch only once in order to (try and) update the policy, in other words participating in reaching consensus and applying it with one atomic write. Solving this problem also has significance in understanding the strength of the SDN switch computation model.

The main idea of our solution is to adjust the previous policy update scheme in a way that performs consensus validation phase during every packet processing thereby requiring the servers only to make the first phase of the consensus - atomically writing the matrix.
In order to allow the packets to validate the consensus we make the following observation: while servers can be abstracted as memory writers, the packets processed by the switch are the readers. OpenFlow standard define an atomic write transaction as being atomic in relation to the processed packets thereby making packet processing a multiple read transaction but one read location per table is allowed. This means that if each memory location used in the consensus scheme will be stored in different table then each packet will be able to read all of them.

However, reading each memory location is not enough, as the scheme requires to make a validation that involves multiple values. We utilize the metadata field in order to store all read values, in more details, each memory value $M_{i,j}$ is stored in offset i+j*n in the metadata field. Once all values are in the metadata field, we can detect the consensus winner by trying different matches on the metadata. Although we can have one match entry per possible matrix state, this solution requires exponential number of entries. A better way which we describe next requires only $O(n^2)$ entries utilizing n tables.

Our compact validation use additional temp variable, $temp_winner$, to hold the current best consensus candidate. Each of the n validating tables checks different "column" in the matrix and update $temp_winner$ to keep track of the winner so far (after examining a prefix sub set of the columns). The entry t in table k $(0<=k<n)$  matches the case where current winner=t and location $(t,k)$ in the matrix (packet header) equals 1 and location (k,k) is non-zero. After all validating tables are processed, $temp_winner$ stores the id of the winning server which can be used to match on its policy rules (filtering out other policies) or to use the goto table command to jump to a designated policy table of the server.


\section{The Practical Implementation}\label{sec:extension}

\subsection{Dealing with multiple updates}

TBD - adding the notion of versions and cleanup/GC. probably no good way to solve infinite versions... maybe check literature on shared memory and multiple writes.


\section{Improving the scheme with advanced OpenFlow features}\label{sec:todo}
Till now we considered a model that allows multiple updates to match-action entries. Here we extends the model with actions create-entry and delete-entry that may failed and abort the whole transaction depending on the existence on non-existence of specific entries prior to the action.
%TBD - much less resources. easily solving the infinite versions issue.

Next we show how such actions can be used to implement consensus with much less resources. We use entry id as a property used to reference specific entry.

\begin{algorithm}[t]
    \caption{Advanced Update Algorithm}
    \label{alg:template}
    \begin{algorithmic}[1]    
    \Require update requests $U$
    \Ensure installed policy is consistent with previous one
    \If {first time}
		%\Do 
		\Repeat 
			\State $my\_cur\_id\gets$ unused entry id 
    		\State $res \gets $ create entry with id $my\_cur\_id$
    	\Until{$res=False$}
    	%\doWhile{$res=False$}
    \EndIf
 		%\Do 
 		\Repeat 
 			\State $cur\_id\gets$ read current entry id 
 			\State $cur\_policy\gets$ read current policy
 			\State $my\_next\_id\gets$ unused entry id 
 			\State $new\_policy\gets$ apply update requests on $cur\_policy$
 			\startTransaction
	 			\State delete entry with id $1<<(\log n) + cur\_id$
	 			\State create entry with id $1<<(\log n) + my\_cur\_id$
	 			\State add $new\_policy$ rules conditioned on $metadata=my\_cur\_id$
	 			\State update policy selector to $my\_cur\_id$
 			\endTransaction
     		\State $res \gets $ commit transaction 
     	\Until{$res=False$}
     	%\doWhile{$res=False$}
%   \vspace{3mm}

    \end{algorithmic}
\end{algorithm}
    

\section{Solving just concurrent policy update without consensus}\label{sec:todo}

TBD - should be simple given two previous sections


\section{Related Work}\label{sec:relwork}

ideas: distributed control planes

ideas: known consensus results and impossibilities

ideas: the power of in-band

\section{Conclusion}\label{sec:conclusion}

ideas: the missing link for STN, but also others

{
\bibliographystyle{abbrv}
\bibliography{references}  % main.bib is the name of the Bibliography in this case
}

\end{document}
