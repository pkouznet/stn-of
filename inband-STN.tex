%\documentclass[11pt,pdftex,letter]{article}
\documentclass[conference]{sigcomm-alternate}

%\documentclass{sig-alternate-10pt}


%\documentclass[11pt]{llncs}
%\documentclass[11pt,pdftex]{article}
%\documentclass{sig-alternate-10pt}
%\usepackage{amsthm}
%\usepackage{algorithm}
%\usepackage[noend]{algorithmic}


\usepackage{amssymb}
\usepackage{comment}
\usepackage{amsmath}
%\usepackage{graphicx}
%\usepackage{color}
%\usepackage[pdftex]{graphicx}
%\DeclareGraphicsRule{*}{mps}{*}{<++>}


%\usepackage[caption=true,font=footnotesize]{subfig}
%\usepackage{xspace} 	% Guesses whether a space is needed when invoked
\usepackage{cite}
\usepackage{url}

%\usepackage{framed}
\usepackage{framed,color}
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}

%\usepackage{algorithm}
%\usepackage[noend]{algorithmic}

\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\algrenewcommand\algorithmicreturn{\State \textbf{return}}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\algdef{SE}[TRANSACT]{StartTransaction}{EndTransaction}{\algorithmicaaa}[1]{\algorithmicwhile\ #1}%

\newcommand{\hide}[1]{}
\algblock[<block>]{<start>}{<end>}
\algblockdefx[Transaction]{startTransaction}{endTransaction} %
[0]{ \textbf{start transaction}} %
[1][res]{ $#1\gets$  \textbf{commit transaction}}

%
%\usepackage{comment}

%[[PKto change spacing
%\usepackage{titlesec}
%\titlespacing\section{0pt}{7pt}{6pt}
%]]

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{takeaway}[theorem]{Takeaway}
%\newtheorem{fact}[theorem]{Fact}


%\pdfpagewidth=8.5in
%\pdfpageheight=11in

% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.


\begin{document}
\sloppy

%\title{Solving Consensus with OpenFlow}

%\title{One-Shot Coordination of Distributed SDN Controllers:\\The Case for Multi-Write Consensus}

%\title{One-Shot Coordination of Distributed SDN Controllers:\\On the Consensus Power of OpenFlow}

%\title{Coordinating Distributed SDN Controllers:\\On the Consensus Power of OpenFlow}

%\title{On the Synchronization Power of OpenFlow}

\title{Coordinating Distributed SDN Controllers\\{\Large On the Synchronization Power of OpenFlow}}


\author{
Liron Schiff\thanks{Supported by the European Research Council (ERC) Starting Grant no. 259085 and by the Israel Science Foundation Grant no.~1386/11.} $^1$,
Stefan Schmid$^2$, Petr Kuznetsov$^3$ \\
\small $^1$ Tel Aviv University, Israel; $^2$ TU Berlin \& T-Labs,
Germany; $^3$ T\'el\'ecom ParisTech, France
}

%\institute{}

\date{}


\maketitle


\thispagestyle{empty}

%\if \SAVESPACE 1
%\setlength{\floatsep}{3pt}
%\setlength{\textfloatsep}{3pt}
%\setlength{\dbltextfloatsep}{3pt}
%\setlength{\intextsep}{3pt}
%\setlength{\abovecaptionskip}{3pt}
%\fi

% A category with the (minimum) three required fields
%\category{C.2.1}{Network Architecture and Design}{Centralized Networks}
%\category{C.2.4}{Distributed Systems}{Network Operating Systems}
%\terms{Measurement, Performance}
%\keywords{}


\begin{abstract}
Control planes of modern Software-Defined Networks (SDNs)
are \emph{distributed systems}: to ensure availability and fault-tolerance,
to improve load-balancing, and to reduce overheads, 
control modules are physically distributed.
%a redundant control plane
%can ensure a high availability, and a spatially distributed
%control plane allows to handle time-critical dataplane
%events close to their origins.
%[[PK this is a contraversial point, not important for us
%Moreover, an SDN may be managed by multiple administrators simultaneously.
%]]
However, a distributed control plane also introduces
new challenges, and in order to guarantee a consistent network operation,
the actions of different controllers may need to be synchronized and coordinated.
%[[PK sounds contradicting to the fact that such systems already exist
%However, not much is known today about how to implement
%such a distributed control plane.
%]]
In this paper, we show that powerful synchronization mechanisms
can be implemented \emph{in band}, simply using today's Openflow switches.
We show how to leverage standard OpenFlow features for providing simple
but general-purpose transactions with all-or-nothing semantics.
We then show that these transactions can be used to reach consensus
among software controllers and, what is more, to resolve
general conflicts between concurrent policy updates in an
effectively atomic way and \emph{on-the-fly}, without affecting the ongoing traffic.
%[[PK
%In particular, we describe the implementation of test\&set primitives
%as well as multi-writer consensus objects which can be used
%to implement atomic transactions with all-or-nothing semantics.
%Interestingly, our approach allows controllers to detect and
%resolve very general notions of conflicts, not only, e.g., conflicts
%due to overlapping flow spaces but also conflicts based on network load.
%]]
\end{abstract}

%\vspace{1cm}

%\begin{center}
%{\bf [Regular paper only]}
%\end{center}
%[[PK I guess not anymore?]]

%\vspace{1cm}

%\begin{center}
%{\emph{Contact Address:}\\Marco Canini, Place Sainte Barbe~2, 1348 Louvain-la-Neuve, Belgium\\Tel: $+$32 10 47 48 32,
%marco.canini@uclouvain.be}
% {\emph{Contact Address:}\\Stefan Schmid, MAR 4-4, Marchstr.~23, 10587 Berlin, Germany\\Tel: $+$49 175 930 98 75,
% stefan.schmid@tu-berlin.de}
%\end{center}


%\newpage

%\begin{center}
%{\bf Regular and student paper: Dan Levin is a full-time student.}
%\end{center}

%\section*{todo before submission}

\section{Introduction}\label{sec:intro}

By consolidating and outsourcing the control over the dataplane switches to a logically
centralized controller, the concept Software-Defined Networking (SDN)
simplifies network management and facilitates faster innovations,
as the (software) control plane is decoupled from the (hardware) data
plane and can evolve independently.
Moreover, Openflow, the standard SDN protocol today, introduces many generalizations,
in terms of traffic engineering, definition of flows, as well as in-band network functionalities,
by relying on a simple match-action paradigm which allows us to define
forwarding rules based not only on Layer-2, but also Layer-3 and Layer-4 (and possibly beyond)

While the logically centralized perspective offered by SDN is attractive,
there is a wide consensus that
the control plane should be physically \emph{distributed}. 
First, in order to provide a high availability as well as a minimal degree of
fault-tolerance, controllers should be redundant~\cite{onix,stn,onos}: a failure
of one controller can be masked by other controllers. Second, it has also been proposed
to distribute controllers \emph{spatially}, in order to handle latency-sensitive and
communication intensive control plane events close to their origin.~\cite{devoflow,kandoo,jukka,disco}
Third, larger SDNs are likely to be operated by multiple administrators or may even offer
participatory interfaces where different users can install and trigger policy changes
concurrently~\cite{participatory,stn}.
All this, however, comes with the need for \emph{synchronization}
and \emph{coordination} among the distributed controllers:
the network should be manipulated \emph{consistently}~\cite{cpc}.
This task is nontrivial~\cite{cap-theorem}.

%[[PK not sure I see why talking about it here
%[[
%A particularly interesting problem regards the consistent installation of new policies
%or routes~\cite{network-update,roger-hotnets,correct,stn}. While implementing consistent network updates
%is challenging already from the perspective of a single controller, there exists a wide consensus
%that the logically centralized SDN control planes will likely be \emph{actually distributed} in the future.
%]]

Today, we do not have a good understanding yet of how to realize
such distributed control planes. The problem is essentially a
distributed systems
one: Multiple controllers may simultaneously try to
install conflicting updates and we want to resolve these conflicts
\emph{consistently} (no undesired behavior is observed on the data
plane) and \emph{efficiently} (no undesired delays are imposed on the
control application).

Consider, for example, the practical problem of
consistent installation of new forwarding policies, stipulating routes
that packets of different header spaces should follow across the
network~\cite{network-update,roger-hotnets,correct,stn}.
Installing conflicting forwarding rules, e.g., rules of the same priority defined over non-disjoint
flow spaces may lead to pathological network behavior (loops,
blackholes, etc.).
Similarly, installing diverging load-balancing policies may, 
when combined, \emph{increase} the load~\cite{log-cent}.
To render things more difficult, controllers may also fail, 
even before their updates have been completed.

%\textbf{} 
\paragraph{Our Contributions}
In this paper, we show that 
OpenFlow~\cite{of-spec}---the standard SDN protocol today---can
be used to provide strong controller synchronization and coordination 
primitives \emph{in-band}.
In particular, we show how to exploit the \emph{bundling}
feature and the $\textsf{OFPFF\_CHECK\_OVERLAP}$ flag,
to implement a \emph{shared memory} abstraction of the switch
and its flow tables: 
This shared memory 
allows us to implement
\emph{transactions} which may
atomically install multiple rules if certain compatibility conditions
are met, and which also facilitate \emph{packet-atomic} rule updates. 
These transactions can be \emph{multi-write} or even \emph{multi-modify}:
while not supporting
read operations, the transaction can be conditioned on the presence of
certain rules.
This ability is reminiscent of the hypothetical multi-write
shared-memory model which is known to enable solutions for important
synchronization problems, such as consensus, impossible to solve
otherwise~\cite{multi-objects}.
Indeed, we describe an algorithm that solves consensus among any
number of controllers using a single OpenFlow switch that accepts
bundle control messages. 

Our results highlight the synchronization power hidden in the
OpenFlow standard. The synchronization mechanisms we describe in this
paper turn out to be crucial for ensuring consistency of network
management on the control plane while preserving consistency of the
traffic processed on the data plane.
As a case study, we also show how to replace the read-modify-write
primitives voluntarily assumed in the recent implementation of a
consistent composition system for forwarding policies~\cite{cpc}.
We also show how to use these transactions directly to implement a lightweight
forwarding policy composition system that requires no additional
synchronization on the control plane.
Finally, we explore how more general classes of policies can be composed
\emph{on-the-switch}.


% we show that powerful synchronization primitives
%can be implemented in Openflow, allowing controllers to implement
%non-trivial atomic transactions, and detect and
%automatically resolve conflicts. In particular, we show how to implement
%the classic distributed computing primitive
%\emph{test\&set}, supporting the conditional installation of new rules,
%and we show how to use the bundle feature to implement
%multi-writer objects, an efficient means to compute consensus;
%interestingly, our implementation can also be used to compute a consensus
%and install the winner's rule \emph{atomically}, using one message.
%Moreover, our approach allows to detect and resolve very general notions of conflicts:
%conflicts may be defined, for example, for rules of the same priority over
%defined over non-independent flow spaces, but may also depend, e.g., on
%network load (based on counters).
%We believe that our approach nicely complements
%existing, more theoretical literature on distributed control planes; for example,
%it also shows how to realize the atomic read-modify-write primitive postulated in
%STN~\cite{stn}.

\paragraph{Organization}
The rest of the paper is organized as follows.
In Section~\ref{sec:model}, we overview the basics of the  OpenFlow
protocol and recall the bundle and \textsf{ofpff\_check\_overlap} features.
In Section~\ref{sec:sync}, we specify the implementations of our OpenFlow-based
sycnhronization mechanisms.
In Section~\ref{sec:application}, we discuss a few policy composition
algorithms that benefit from these mechanisms.
In Section~\ref{sec:conclusion}, we discuss open questions and related work.

%we provide the theoretical
%background on our approach. Section~\ref{sec:realization}
%then discusses the basic implementation of this approach in Openflow.
%Section~\ref{sec:extensions} discusses practical issues and extensions.
%After reviewing related literature in Section~\ref{sec:relwork},
%we conclude our work in
%Section~\ref{sec:conclusion}.

\section{OpenFlow Background}\label{sec:model}

The synchronization and coordination mechanisms presented in this paper
are based on Openflow~\cite{of-spec}, the standard SDN protocol today. In this section, 
we provide the necessary background. 


es a small partial deployment of SDN and
out-sources the control over a set of Openflow switches to a logically centralized software controller.
Openflow, the standard SDN protocol today, is based on a match-action paradigm: the controller
can install simple rules on the Openflow switches.

In a nutshell, 
OpenFlow
is based on a match-action concept: OpenFlow switches store
rules (installed by the controller) consisting of a match and an
action part. A packet matched by a certain rule will be subject
to the associated action (to be immediately applied now or later). 
For example, an action can define a port to which the
matched packet should be forwarded, or add or change a tag
(a certain part in the packet header).
Each OpenFlow switch stores one or multiple flow tables,
each of which contains a set of rules (flow entries). Flow
tables form a pipeline, and flow entries are ordered according
to priorities: A packet arriving at a switch is first checked by
the rule of the highest priority in table 0: the fields of the
data packet are compared with the match fields of that rule,
and if they fit, some instructions (the actions) are executed;
subsequently, lower priority rules are checked. Depending on
the outcome of the table 0 processing, the packet may be sent
to additional flow tables in the pipeline; during the traversal of
the flow tables, the packet can be equipped with meta-tags to
transfer temporary information. Concretely, instructions can be
used to define additional tables to be visited (goto instruction),
to modify the set of to-be-applied actions (either by appending,
deleting, or modifying actions), or immediately apply some
actions to the packet. A meta-data field can be used to exchange
information between tables; it is part of the header that can be
inserted or removed from a packet via push and pop actions.
In general, a packet can be matched against any of its header
fields, and fields can be wildcarded and sometimes bitmasked.
(For instance, the meta-data field is maskable.) If no rule
matches, the packet is dropped. 

\textbf{TODO LIRON: what else is missing? describe cookies?}

The Openflow standard (as of \textbf{TODO}) offers two interesting
features which we will exploit to implement the transactional memory abstraction:

\begin{enumerate}
\item \textbf{Bundling:}
Control messages are aggregated into one message, so that either all of the control messages are
processed, or none of them is. In particular, if, for some reason, the
configuration request contained in one of the bundled messages cannot
be processed, all the other requests  are rejected and an error
message is sent to the controller that issued them.
Interestingly, this bundled processing can be, when properly configured, done in a
\emph{packet-atomic} way, so that each packet is processed according
to the switch configuration either \emph{before} or \emph{after} the
request was processed.

\item \textbf{Conditional Application:}
The Openflow standard also supports the \emph{conditional} application of
certain types of control messages. A specific $\textsf{OFPFF\_CHECK\_OVERLAP}$ flag
set in the control message containing a new forwarding rule ensures
that the rule is applied if and only if it does not \emph{conflict}
with any other rule currently in the flow table.
This in-band conflict detection mechanism, combined with bundling,
allows us to implement specific types of \emph{transactions} that may
atomically install multiple rules if certain compatibility conditions
are met.
\end{enumerate}

\section{Vision and Problem Statement}\label{sec:vision}

This paper shows how to implement a transactional memory abstraction using
standard Openflow switches. The shared memory abstraction of the 
flow table at the switch allows multiple controllers to atomically and 
concurrently read and modify state,
using the bundling feature to implement \emph{multi-write} or, more
widely, \emph{multi-modify} transactions. .
This ability allows us to solve consensus among any
number of controllers, using a single OpenFlow switch that accepts
bundle control messages.

These synchronization mechanisms can be used to
achieve consistent network management.
We first show that they can be used to replace the read-modify-write
primitives voluntarily assumed in the recent implementation of a
recent proposal for a consistent composition system for forwarding policies~\cite{cpc}.
We then use these transactions directly to implement a lightweight
forwarding policy composition system that requires no additional
synchronization on the control plane.
We then explore how more general classes of policies can be composed
\emph{on-the-switch}.

TODO: write vision and contribution in full detail, as an overview


\section{Transactions in OpenFlow}\label{sec:sync}

Multi-write and read-modify transactions.

\subsection{Dealing with multiple updates}

TBD - adding the notion of versions and cleanup/GC. probably no good way to solve infinite versions... maybe check literature on shared memory and multiple writes.


\subsection{Improving the scheme with check-overlap}\label{sec:todo}

Till now we considered a model that allows multiple updates to match-action entries. Here we extends the model with actions create-entry and delete-entry that may failed and abort the whole transaction depending on the existence on non-existence of specific entries prior to the action.
%TBD - much less resources. easily solving the infinite versions issue.

Next we show how such actions can be used to implement consensus with much less resources. We use entry id as a property used to reference specific entry.


\begin{algorithm}[t]
    \caption{Advanced Update Algorithm}
    \label{alg:template}
    \begin{algorithmic}[1]
    \Require update requirements $U$, two entries containers CUR and NEXT.
    \Ensure installed policy is consistent with previous one
    \Repeat
			\State $my\_id\gets$ unused id (in both CUR and NEXT)
    		\State $res \gets $ NEXT.Add($my\_id$, True)
    \Until{$res=True$}
 		%\Do
 		\Repeat
 			\State $pid\gets$ read current policy id (only one in CUR)
 			\State NEXT.Add($pid$)
 			\If {$pid\notin CUR$}
 				\State {\bf continue} (restart loop)
 			\EndIf
 			\State $cur\_policy\gets$ read current policy
 			\State $new\_policy\gets apply\_update(cur\_policy,U)$
 			\startTransaction
	 			\State CUR.Delete($pid$)
	 			\State CUR.Add($my\_id$)
				\State NEXT.Delete($my\_id$, True)
				\State NEXT.Delete($pid$)
	 			\State add $new\_policy$ rules %conditioned on $metadata=my\_a$
	 			%\State update policy selector to $my\_a$
 			\endTransaction
     		%\State $res \gets $ commit transaction
     	\Until{$res=True$}
     	%\doWhile{$res=False$}
%   \vspace{3mm}
			\Return

    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \caption{Container Add abstraction}
    \label{alg:template}
    \begin{algorithmic}[1]
    \Require Container id $C$, entry id $i$, unique constrain (defaults to False) $b\_unique$, caller id $caller$.
%    \Ensure
    		\State $value \gets C.i$
    		\State $mask \gets all-ones$
    		\State $cookie \gets C.i.caller$
    		\State $action \gets C.i.caller$
    		\If {$b\_unique$}
    			\State $flag \gets 0$
    		\Else
    		    \State $flag \gets CHECK\_OVERLAP$
    		\EndIf
    					\State $cmd\gets FlowMod(match=(value,mask), operation = ADD, cookie, flag, action) $
			\Return cmd
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \caption{Container Delete abstraction}
    \label{alg:template}
    \begin{algorithmic}[1]
    \Require Container id $C$, entry id $i$, unique constrain (defaults to False) $b\_unique$, caller id $caller$.
%    \Ensure
    		\State $value \gets C.i$
    		\State $mask \gets all-ones$
    		\State $cookie \gets C.i.caller$
    		\If {$b\_unique$}
    			\State $cmd\gets FlowMod(match=(value,mask), operation=DELETE) $
    		\Else
    		    \State $cmd\gets FlowMod(match=(value,mask), operation=DELETE_STRICT, cookie) $
    		\EndIf
			
			\Return cmd
    \end{algorithmic}
\end{algorithm}

\hide{
\begin{algorithm}[t]
    \caption{Advanced Update Algorithm2}
    \label{alg:template}
    \begin{algorithmic}[1]
    \Require update requirements $U$, two entries containers CUR and NEXT.
    \Ensure installed policy is consistent with previous one
    \If {first time}
		%\Do
		\Repeat
			\State $my\_id\gets$ unused id (in both CUR and NEXT)
    		\State $res \gets $ NEXT.Add($my\_id$, True)
    	\Until{$res=True$}
    	%\doWhile{$res=False$}
    \EndIf
 		%\Do
 		\Repeat
 			\State $pid\gets$ read current policy id (only one in CUR)
 			\State NEXT.Add($pid$)
 			\If {$pid\notin CUR$}
 				\State {\bf continue} (restart loop)
 			\EndIf
 			\State $cur\_policy\gets$ read current policy
 			\State $next\_id\gets$  unused id (in both CUR and NEXT)
 			\State $new\_policy\gets apply\_update(cur\_policy,U)$
 			\startTransaction
	 			\State CUR.Delete($b_pid$)
	 			\State CUR.Add($my\_id$)
				\State NEXT.Delete($my\_id$, True)
				\State NEXT.Add($next\_id$, True)
				\State NEXT.Delete($pid$)
	 			\State add $new\_policy$ rules %conditioned on $metadata=my\_a$
	 			%\State update policy selector to $my\_a$
 			\endTransaction
     		%\State $res \gets $ commit transaction
     	\Until{$res=True$}
     	\State $my\_id\gets next\_id$
     	%\doWhile{$res=False$}
%   \vspace{3mm}

			\Return
    \end{algorithmic}
\end{algorithm}
}


\section{Applications and ramifications}\label{sec:application}



\section{The Underlying Theory}\label{sec:background}

idea: multi-write consensus, a not well-known result!

In order to solve consensus we suggest to use the atomic multiple entries update capability (which is discussed in OpenFlow standard). A shared memory system that supports atomic write to (enough) multiple locations is known to allow the implementation of consensus objects. For example consider the following implementation utilizing $n^2$ memory locations, $M_{i,j}$, where $i,j\in[n]$, and the suggested values $\{v_i\}_{i\in [n]}$. All locations are initialized to zero.

In order to participate, a server i, writes its suggested value $V_i$ and atomically rewrites row i ($M_{i,*}$) with value "1" and column i ($M_{*,i}$) with value "2". After the atomic write, a server $i$ first reads the diagonal ($M_{j,j}$ for $j\in [n]$), and consider all observed servers as candidate set S.  Then server i reads all candidates intersections (locations $M_{j,k}$ where $j,k\in S$. And computes the winner server id w that was overwritten by all other candidates, i.e. such that $M_{w,k}=2$ for all $k\in S {w}$. The consensus value is $v_w$.
Note that a snapshot could replace the two reading phases.



\section{Basic One-Touch Mechanism}\label{sec:realization}

We address the problem of committing consistent policies to SDN switch concurrently by multiple controllers.
We show that assuming minimalistic control protocol, SDN switch configuration space can be used to implement a consensus object thereby allowing controllers to agree on each next policy update.

\subsection{SDN switch configuration space as shared memory}





Our first observation is that a match-action configuration which is accessed by multiple controllers can be considered as shared memory. Note however that standard memory map and index/offset to a word, while Flow Table entries are not necessarily indexed. However, we assume that each flow entry has some property that allows the operator to specifically reference and update it, for example OpenFlow entry's cookie. In cases where this property value can't be determined by the operator, we can use other variable entry properties to store the index, for example we can represent memory value $v$ in offset $i$ by an match-action entry whose match is $in\_port = i$ and action is "forward port $x$".
%To overcome this we can use the match part of an entry as the index and the action part as the word value.


Going back to the policy update problem, the consensus scheme allow all servers to decide on next policy by using consensus value as policy identifier, and writing the policy in advance. However this scheme doesn't actually apply the policy on incoming packets and it remains to configure the switch according to the next policy once it is decided. Making this scheme fail safe (handling failure of winning server) can be ensured by allowing any server to reconfigure the switch.

\subsection{ One-Touch}

In some scenarios it may be desirable to shorten the policy update time by avoiding the consensus reading and policy configuration step. We define this as the one touch policy update, namely we require each server to contact the switch only once in order to (try and) update the policy, in other words participating in reaching consensus and applying it with one atomic write. Solving this problem also has significance in understanding the strength of the SDN switch computation model.

The main idea of our solution is to adjust the previous policy update scheme in a way that performs consensus validation phase during every packet processing thereby requiring the servers only to make the first phase of the consensus - atomically writing the matrix.
In order to allow the packets to validate the consensus we make the following observation: while servers can be abstracted as memory writers, the packets processed by the switch are the readers. OpenFlow standard define an atomic write transaction as being atomic in relation to the processed packets thereby making packet processing a multiple read transaction but one read location per table is allowed. This means that if each memory location used in the consensus scheme will be stored in different table then each packet will be able to read all of them.

However, reading each memory location is not enough, as the scheme requires to make a validation that involves multiple values. We utilize the metadata field in order to store all read values, in more details, each memory value $M_{i,j}$ is stored in offset i+j*n in the metadata field. Once all values are in the metadata field, we can detect the consensus winner by trying different matches on the metadata. Although we can have one match entry per possible matrix state, this solution requires exponential number of entries. A better way which we describe next requires only $O(n^2)$ entries utilizing n tables.

Our compact validation use additional temp variable, $temp_winner$, to hold the current best consensus candidate. Each of the n validating tables checks different "column" in the matrix and update $temp_winner$ to keep track of the winner so far (after examining a prefix sub set of the columns). The entry t in table k $(0<=k<n)$  matches the case where current winner=t and location $(t,k)$ in the matrix (packet header) equals 1 and location (k,k) is non-zero. After all validating tables are processed, $temp_winner$ stores the id of the winning server which can be used to match on its policy rules (filtering out other policies) or to use the goto table command to jump to a designated policy table of the server.


\section{The Practical Implementation}\label{sec:extension}



\section{Related Work}\label{sec:relwork}

There exists a wide consensus that SDN control planes need to be distributed.~\cite{onos,onix,elasticon}
Onix~\cite{onix} is one of the earliest proposals, and introduced the
the notion of Network Information Base (NIB), abstracting network state
distribution from control logic, but
requiring mechanisms for the detection and resolution of conflicts.
Also spatially distributed control planes to improve scalability and
latency have been studied intensively
in the literature~\cite{kandoo,ctrl-place,hotsdn13loc}
proposes an elastic distributed controller architecture.

Operating and updating SDNs consistently is already non-trivial
from the perspective of a single controller. Reitblatt et al.~~\cite{network-update}
introduced the notion of
per-packet consistency and proposed a 2-phase update protocol based on tagging.
%, and described the \emph{two-phase update} technique, also used in our algorithms.
Mahajan and Wattenhofer~\cite{roger-hotnets} considered weaker transient
consistency guarantees, and proposed more efficient network update algorithms
accordingly. Ludwig et al.~~\cite{hotnets14update} studied algorithms for secure
network updates where packets are forced to traverse certain waypoints or
middleboxes. Ghorbani et al.~~\cite{correct-virt} recently argued for the design
of network update algorithms that provide even stronger consistency guarantees.

To the best of our knowledge, the only paper explicitly addressing the consistent
network update problem from a distributed controller perspective is STN~\cite{stn}:
STN provides a transactional interface offering all-or-nothing semantics and serializability
(the ``holy grail'' of safety properties),
allowing
controllers to install policies in a conflict-free manner; if some controllers fail,
other controllers can take over. STN~\cite{stn} is based on
atomic
read-modify-write primitives, but while this primitive is postulated, no implementation is
described. In this paper, we provide this missing link, and also show that other useful and
much more powerful synchronization primitives can be implemented.

Our work is also closely related to the recent work on providing more high-level
abstractions and programming languages for SDNs, such as Frenetic~\cite{frenetic},
also enabling policy composition~\cite{pyretic}.

In terms of distributed computing: TODO PETR: maybe modify a bit the text below
and also write more about multi-writer consensus etc.?

\noindent\textbf{Distributed Computing.}
There is a long tradition of defining correctness of a concurrent system via
an equivalence to a sequential one~\cite{Pap79-serial,Lam79,HW90}.  The notion
of sequentially composable histories is reminiscent of
linearizability~\cite{HW90}, where a history of operations concurrently
applied by a collection of processes is equivalent to a history in which the
operations are in a sequential order, respecting their real-time precedence.
In contrast, our sequentially composable histories impose requirements not
only on high-level invocations and responses, but also on the way the traffic
is processed. We require that the committed policies constitute a
conflict-free sequential history, but, additionally,  we expect that each
\emph{path} witnesses only a prefix of this history, consisting of all
requests that were committed before the path was initiated.
%
The transactional interface exported by the CPC abstraction is inspired by the
work on speculative concurrency control using software transactional memory
(STM)~\cite{stm-st95}.
%, thus the  term \emph{software transactional networking}.
Our interface is however intended to model realistic network
management operations, which makes it simpler than recent
dynamic STM models~\cite{dstm}.
%In particular, the sets of rules to be installed by a policy update do not depend on the state of the
%network.

Extending the interface to dynamic policies that adapt their behavior based on
the current network state sounds like a promising research direction.  On the
other hand, our criterion of sequential composability is more complex than
traditional STM correctness properties in that it imposes restrictions not only
on the high-level interface exported to the control plane, but also on the
paths taken by the data-plane packets.
Also, we assumed that controllers are subject to failures, which is usually not
assumed by STM implementations.


\section{Conclusion}\label{sec:conclusion}

This paper showed that powerful synchronization primitives
from distributed computing can be implemented in Openflow.
Our work complements existing research on the design of
distributed control planes, and also provides some missing links,
e.g., for the transactional approach taken by STN.
We also hope that our work can nourish the ongoing discussion of
what additional in-band features may be useful for future
versions of Openflow.

{
\bibliographystyle{abbrv}
\bibliography{references}  % main.bib is the name of the Bibliography in this case
}

\end{document}
